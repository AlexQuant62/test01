{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "Class definition for callbacks from:\n",
    "    https://github.com/ray-project/ray/blob/0c80efa2a37f482494fbffbe9e81f61586b03ecb/rllib/examples/custom_metrics_and_callbacks.py\n",
    "- Modified to add custom metrics for TensorBoard (max/mean/min) : net_worth, num_batches (added by default).\n",
    "- Debug prints commented.\n",
    "- During debugging it was found that sometimes episode.last_info_for() return None\n",
    "- Don`t know whether it worth it to add these metrics, in my testing (DQN) i receive warnings that 'process_trial' and 'experiment_checkpoint'\n",
    "operations take 1-2 sec to complete on each iteration which more than double training time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example of using RLlib's debug callbacks.\"\"\"\n",
    "\n",
    "from typing import Dict\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                         policies: Dict[str, Policy],\n",
    "                         episode: MultiAgentEpisode, **kwargs):\n",
    "#        print(\"episode {} started\".format(episode.episode_id))\n",
    "        episode.user_data[\"net_worth\"] = []\n",
    "        episode.hist_data[\"net_worth\"] = []\n",
    "\n",
    "    def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode, **kwargs):\n",
    "        info = episode.last_info_for()\n",
    "        if info is not None: # why None??\n",
    "            net_worth = info['net_worth']\n",
    "            episode.user_data[\"net_worth\"].append(net_worth)\n",
    "\n",
    "    def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "                       **kwargs):\n",
    "        net_worth = np.mean(episode.user_data[\"net_worth\"])\n",
    "#        print(\"episode {} ended with length {} and net worth {}\".format(\n",
    "#            episode.episode_id, episode.length, net_worth))\n",
    "        episode.custom_metrics[\"net_worth\"] = net_worth\n",
    "        episode.hist_data[\"net_worth\"] = episode.user_data[\"net_worth\"]\n",
    "\n",
    "    def on_sample_end(self, worker: RolloutWorker, samples: SampleBatch,\n",
    "                      **kwargs):\n",
    "        pass                      \n",
    "#        print(\"returned sample batch of size {}\".format(samples.count))\n",
    "\n",
    "    def on_train_result(self, trainer, result: dict, **kwargs):\n",
    "#        print(\"trainer.train() result: {} -> {} episodes\".format(\n",
    "#            trainer, result[\"episodes_this_iter\"]))\n",
    "        # you can mutate the result dict to add new fields to return\n",
    "        result[\"callback_ok\"] = True\n",
    "\n",
    "    def on_postprocess_trajectory(\n",
    "            self, worker: RolloutWorker, episode: MultiAgentEpisode,\n",
    "            agent_id: str, policy_id: str, policies: Dict[str, Policy],\n",
    "            postprocessed_batch: SampleBatch,\n",
    "            original_batches: Dict[str, SampleBatch], **kwargs):\n",
    "#        print(\"postprocessed {} steps\".format(postprocessed_batch.count))\n",
    "        if \"num_batches\" not in episode.custom_metrics:\n",
    "            episode.custom_metrics[\"num_batches\"] = 0\n",
    "        episode.custom_metrics[\"num_batches\"] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import tensortrade.env.default as default\n",
    "\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.instruments import Instrument\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "\n",
    "\n",
    "USD = Instrument(\"USD\", 2, \"U.S. Dollar\")\n",
    "TTC = Instrument(\"TTC\", 8, \"TensorTrade Coin\")\n",
    "\n",
    "\n",
    "def create_env(config):\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1000)\n",
    "    p = Stream.source(50*np.sin(3*x) + 100, dtype=\"float\").rename(\"USD-TTC\")\n",
    "\n",
    "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
    "        p\n",
    "    )\n",
    "\n",
    "    cash = Wallet(bitfinex, 100000 * USD)\n",
    "    asset = Wallet(bitfinex, 0 * TTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "    feed = DataFeed([\n",
    "        p,\n",
    "        p.rolling(window=10).mean().rename(\"fast\"),\n",
    "        p.rolling(window=50).mean().rename(\"medium\"),\n",
    "        p.rolling(window=100).mean().rename(\"slow\"),\n",
    "        p.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "\n",
    "    reward_scheme = default.rewards.PBR(price=p)\n",
    "\n",
    "    action_scheme = default.actions.BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    env = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    return env\n",
    "\n",
    "register_env(\"TradingEnv\", create_env)\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "      \"episode_reward_mean\": 500\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"TradingEnv\",\n",
    "        \"callbacks\": MyCallbacks, #callbacks added\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", \"max\"),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"callbacks\": MyCallbacks, # callbacks added\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
